{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "This script is based on the preprocessing script that Mughda made (https://github.com/seraphinatarrant/embedding_bias/tree/Mugdha). I moved all of the code to ``data_cleaning`` so that it could be more easily accessible in case one of the other scripts needs it. Other than that, I only changed the code so that it would also be usable with csv files.\n",
    "\n",
    "As for the structure of the data, in most of the other scripts I will assume that you saved the data to ``data/archives/2019_03``. If you want to change this, you will have to change that from all of the other scripts. Note that the rest of the code should not have any of that hardcoded for ease of code recycling.\n",
    "\n",
    "We first import all of the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import Preprocessing as pre\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is only for debugging and should be removed before uploading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = reload(pre)\n",
    "preprocess_file = pre.preprocess_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean and preprocess the embeddings dataset. This also generates the vocabulary file that will be used for cleaning the downstream dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "10000 of 715313 Tweets cleaned\n",
      "20000 of 715313 Tweets cleaned\n",
      "30000 of 715313 Tweets cleaned\n",
      "40000 of 715313 Tweets cleaned\n",
      "50000 of 715313 Tweets cleaned\n",
      "60000 of 715313 Tweets cleaned\n",
      "70000 of 715313 Tweets cleaned\n",
      "80000 of 715313 Tweets cleaned\n",
      "90000 of 715313 Tweets cleaned\n",
      "100000 of 715313 Tweets cleaned\n",
      "110000 of 715313 Tweets cleaned\n",
      "120000 of 715313 Tweets cleaned\n",
      "130000 of 715313 Tweets cleaned\n",
      "140000 of 715313 Tweets cleaned\n",
      "150000 of 715313 Tweets cleaned\n",
      "160000 of 715313 Tweets cleaned\n",
      "170000 of 715313 Tweets cleaned\n",
      "180000 of 715313 Tweets cleaned\n",
      "190000 of 715313 Tweets cleaned\n",
      "200000 of 715313 Tweets cleaned\n",
      "210000 of 715313 Tweets cleaned\n",
      "220000 of 715313 Tweets cleaned\n",
      "230000 of 715313 Tweets cleaned\n",
      "240000 of 715313 Tweets cleaned\n",
      "250000 of 715313 Tweets cleaned\n",
      "260000 of 715313 Tweets cleaned\n",
      "270000 of 715313 Tweets cleaned\n",
      "280000 of 715313 Tweets cleaned\n",
      "290000 of 715313 Tweets cleaned\n",
      "300000 of 715313 Tweets cleaned\n",
      "310000 of 715313 Tweets cleaned\n",
      "320000 of 715313 Tweets cleaned\n",
      "330000 of 715313 Tweets cleaned\n",
      "340000 of 715313 Tweets cleaned\n",
      "350000 of 715313 Tweets cleaned\n",
      "360000 of 715313 Tweets cleaned\n",
      "370000 of 715313 Tweets cleaned\n",
      "380000 of 715313 Tweets cleaned\n",
      "390000 of 715313 Tweets cleaned\n",
      "400000 of 715313 Tweets cleaned\n",
      "410000 of 715313 Tweets cleaned\n",
      "420000 of 715313 Tweets cleaned\n",
      "430000 of 715313 Tweets cleaned\n",
      "440000 of 715313 Tweets cleaned\n",
      "450000 of 715313 Tweets cleaned\n",
      "460000 of 715313 Tweets cleaned\n",
      "470000 of 715313 Tweets cleaned\n",
      "480000 of 715313 Tweets cleaned\n",
      "490000 of 715313 Tweets cleaned\n",
      "500000 of 715313 Tweets cleaned\n",
      "510000 of 715313 Tweets cleaned\n",
      "520000 of 715313 Tweets cleaned\n",
      "530000 of 715313 Tweets cleaned\n",
      "540000 of 715313 Tweets cleaned\n",
      "550000 of 715313 Tweets cleaned\n",
      "560000 of 715313 Tweets cleaned\n",
      "570000 of 715313 Tweets cleaned\n",
      "580000 of 715313 Tweets cleaned\n",
      "590000 of 715313 Tweets cleaned\n",
      "600000 of 715313 Tweets cleaned\n",
      "610000 of 715313 Tweets cleaned\n",
      "620000 of 715313 Tweets cleaned\n",
      "630000 of 715313 Tweets cleaned\n",
      "640000 of 715313 Tweets cleaned\n",
      "650000 of 715313 Tweets cleaned\n",
      "660000 of 715313 Tweets cleaned\n",
      "670000 of 715313 Tweets cleaned\n",
      "680000 of 715313 Tweets cleaned\n",
      "690000 of 715313 Tweets cleaned\n",
      "700000 of 715313 Tweets cleaned\n",
      "710000 of 715313 Tweets cleaned\n",
      "715313 of 715313 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "217598 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 715313 Tweets processed\n",
      "2000 of 715313 Tweets processed\n",
      "3000 of 715313 Tweets processed\n",
      "4000 of 715313 Tweets processed\n",
      "5000 of 715313 Tweets processed\n",
      "6000 of 715313 Tweets processed\n",
      "7000 of 715313 Tweets processed\n",
      "8000 of 715313 Tweets processed\n",
      "9000 of 715313 Tweets processed\n",
      "10000 of 715313 Tweets processed\n",
      "11000 of 715313 Tweets processed\n",
      "12000 of 715313 Tweets processed\n",
      "13000 of 715313 Tweets processed\n",
      "14000 of 715313 Tweets processed\n",
      "15000 of 715313 Tweets processed\n",
      "16000 of 715313 Tweets processed\n",
      "17000 of 715313 Tweets processed\n",
      "18000 of 715313 Tweets processed\n",
      "19000 of 715313 Tweets processed\n",
      "20000 of 715313 Tweets processed\n",
      "21000 of 715313 Tweets processed\n",
      "22000 of 715313 Tweets processed\n",
      "23000 of 715313 Tweets processed\n",
      "24000 of 715313 Tweets processed\n",
      "25000 of 715313 Tweets processed\n",
      "26000 of 715313 Tweets processed\n",
      "27000 of 715313 Tweets processed\n",
      "28000 of 715313 Tweets processed\n",
      "29000 of 715313 Tweets processed\n",
      "30000 of 715313 Tweets processed\n",
      "31000 of 715313 Tweets processed\n",
      "32000 of 715313 Tweets processed\n",
      "33000 of 715313 Tweets processed\n",
      "34000 of 715313 Tweets processed\n",
      "35000 of 715313 Tweets processed\n",
      "36000 of 715313 Tweets processed\n",
      "37000 of 715313 Tweets processed\n",
      "38000 of 715313 Tweets processed\n",
      "39000 of 715313 Tweets processed\n",
      "40000 of 715313 Tweets processed\n",
      "41000 of 715313 Tweets processed\n",
      "42000 of 715313 Tweets processed\n",
      "43000 of 715313 Tweets processed\n",
      "44000 of 715313 Tweets processed\n",
      "45000 of 715313 Tweets processed\n",
      "46000 of 715313 Tweets processed\n",
      "47000 of 715313 Tweets processed\n",
      "48000 of 715313 Tweets processed\n",
      "49000 of 715313 Tweets processed\n",
      "50000 of 715313 Tweets processed\n",
      "51000 of 715313 Tweets processed\n",
      "52000 of 715313 Tweets processed\n",
      "53000 of 715313 Tweets processed\n",
      "54000 of 715313 Tweets processed\n",
      "55000 of 715313 Tweets processed\n",
      "56000 of 715313 Tweets processed\n",
      "57000 of 715313 Tweets processed\n",
      "58000 of 715313 Tweets processed\n",
      "59000 of 715313 Tweets processed\n",
      "60000 of 715313 Tweets processed\n",
      "61000 of 715313 Tweets processed\n",
      "62000 of 715313 Tweets processed\n",
      "63000 of 715313 Tweets processed\n",
      "64000 of 715313 Tweets processed\n",
      "65000 of 715313 Tweets processed\n",
      "66000 of 715313 Tweets processed\n",
      "67000 of 715313 Tweets processed\n",
      "68000 of 715313 Tweets processed\n",
      "69000 of 715313 Tweets processed\n",
      "70000 of 715313 Tweets processed\n",
      "71000 of 715313 Tweets processed\n",
      "72000 of 715313 Tweets processed\n",
      "73000 of 715313 Tweets processed\n",
      "74000 of 715313 Tweets processed\n",
      "75000 of 715313 Tweets processed\n",
      "76000 of 715313 Tweets processed\n",
      "77000 of 715313 Tweets processed\n",
      "78000 of 715313 Tweets processed\n",
      "79000 of 715313 Tweets processed\n",
      "80000 of 715313 Tweets processed\n",
      "81000 of 715313 Tweets processed\n",
      "82000 of 715313 Tweets processed\n",
      "83000 of 715313 Tweets processed\n",
      "84000 of 715313 Tweets processed\n",
      "85000 of 715313 Tweets processed\n",
      "86000 of 715313 Tweets processed\n",
      "87000 of 715313 Tweets processed\n",
      "88000 of 715313 Tweets processed\n",
      "89000 of 715313 Tweets processed\n",
      "90000 of 715313 Tweets processed\n",
      "91000 of 715313 Tweets processed\n",
      "92000 of 715313 Tweets processed\n",
      "93000 of 715313 Tweets processed\n",
      "94000 of 715313 Tweets processed\n",
      "95000 of 715313 Tweets processed\n",
      "96000 of 715313 Tweets processed\n",
      "97000 of 715313 Tweets processed\n",
      "98000 of 715313 Tweets processed\n",
      "99000 of 715313 Tweets processed\n",
      "100000 of 715313 Tweets processed\n",
      "101000 of 715313 Tweets processed\n",
      "102000 of 715313 Tweets processed\n",
      "103000 of 715313 Tweets processed\n",
      "104000 of 715313 Tweets processed\n",
      "105000 of 715313 Tweets processed\n",
      "106000 of 715313 Tweets processed\n",
      "107000 of 715313 Tweets processed\n",
      "108000 of 715313 Tweets processed\n",
      "109000 of 715313 Tweets processed\n",
      "110000 of 715313 Tweets processed\n",
      "111000 of 715313 Tweets processed\n",
      "112000 of 715313 Tweets processed\n",
      "113000 of 715313 Tweets processed\n",
      "114000 of 715313 Tweets processed\n",
      "115000 of 715313 Tweets processed\n",
      "116000 of 715313 Tweets processed\n",
      "117000 of 715313 Tweets processed\n",
      "118000 of 715313 Tweets processed\n",
      "119000 of 715313 Tweets processed\n",
      "120000 of 715313 Tweets processed\n",
      "121000 of 715313 Tweets processed\n",
      "122000 of 715313 Tweets processed\n",
      "123000 of 715313 Tweets processed\n",
      "124000 of 715313 Tweets processed\n",
      "125000 of 715313 Tweets processed\n",
      "126000 of 715313 Tweets processed\n",
      "127000 of 715313 Tweets processed\n",
      "128000 of 715313 Tweets processed\n",
      "129000 of 715313 Tweets processed\n",
      "130000 of 715313 Tweets processed\n",
      "131000 of 715313 Tweets processed\n",
      "132000 of 715313 Tweets processed\n",
      "133000 of 715313 Tweets processed\n",
      "134000 of 715313 Tweets processed\n",
      "135000 of 715313 Tweets processed\n",
      "136000 of 715313 Tweets processed\n",
      "137000 of 715313 Tweets processed\n",
      "138000 of 715313 Tweets processed\n",
      "139000 of 715313 Tweets processed\n",
      "140000 of 715313 Tweets processed\n",
      "141000 of 715313 Tweets processed\n",
      "142000 of 715313 Tweets processed\n",
      "143000 of 715313 Tweets processed\n",
      "144000 of 715313 Tweets processed\n",
      "145000 of 715313 Tweets processed\n",
      "146000 of 715313 Tweets processed\n",
      "147000 of 715313 Tweets processed\n",
      "148000 of 715313 Tweets processed\n",
      "149000 of 715313 Tweets processed\n",
      "150000 of 715313 Tweets processed\n",
      "151000 of 715313 Tweets processed\n",
      "152000 of 715313 Tweets processed\n",
      "153000 of 715313 Tweets processed\n",
      "154000 of 715313 Tweets processed\n",
      "155000 of 715313 Tweets processed\n",
      "156000 of 715313 Tweets processed\n",
      "157000 of 715313 Tweets processed\n",
      "158000 of 715313 Tweets processed\n",
      "159000 of 715313 Tweets processed\n",
      "160000 of 715313 Tweets processed\n",
      "161000 of 715313 Tweets processed\n",
      "162000 of 715313 Tweets processed\n",
      "163000 of 715313 Tweets processed\n",
      "164000 of 715313 Tweets processed\n",
      "165000 of 715313 Tweets processed\n",
      "166000 of 715313 Tweets processed\n",
      "167000 of 715313 Tweets processed\n",
      "168000 of 715313 Tweets processed\n",
      "169000 of 715313 Tweets processed\n",
      "170000 of 715313 Tweets processed\n",
      "171000 of 715313 Tweets processed\n",
      "172000 of 715313 Tweets processed\n",
      "173000 of 715313 Tweets processed\n",
      "174000 of 715313 Tweets processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175000 of 715313 Tweets processed\n",
      "176000 of 715313 Tweets processed\n",
      "177000 of 715313 Tweets processed\n",
      "178000 of 715313 Tweets processed\n",
      "179000 of 715313 Tweets processed\n",
      "180000 of 715313 Tweets processed\n",
      "181000 of 715313 Tweets processed\n",
      "182000 of 715313 Tweets processed\n",
      "183000 of 715313 Tweets processed\n",
      "184000 of 715313 Tweets processed\n",
      "185000 of 715313 Tweets processed\n",
      "186000 of 715313 Tweets processed\n",
      "187000 of 715313 Tweets processed\n",
      "188000 of 715313 Tweets processed\n",
      "189000 of 715313 Tweets processed\n",
      "190000 of 715313 Tweets processed\n",
      "191000 of 715313 Tweets processed\n",
      "192000 of 715313 Tweets processed\n",
      "193000 of 715313 Tweets processed\n",
      "194000 of 715313 Tweets processed\n",
      "195000 of 715313 Tweets processed\n",
      "196000 of 715313 Tweets processed\n",
      "197000 of 715313 Tweets processed\n",
      "198000 of 715313 Tweets processed\n",
      "199000 of 715313 Tweets processed\n",
      "200000 of 715313 Tweets processed\n",
      "201000 of 715313 Tweets processed\n",
      "202000 of 715313 Tweets processed\n",
      "203000 of 715313 Tweets processed\n",
      "204000 of 715313 Tweets processed\n",
      "205000 of 715313 Tweets processed\n",
      "206000 of 715313 Tweets processed\n",
      "207000 of 715313 Tweets processed\n",
      "208000 of 715313 Tweets processed\n",
      "209000 of 715313 Tweets processed\n",
      "210000 of 715313 Tweets processed\n",
      "211000 of 715313 Tweets processed\n",
      "212000 of 715313 Tweets processed\n",
      "213000 of 715313 Tweets processed\n",
      "214000 of 715313 Tweets processed\n",
      "215000 of 715313 Tweets processed\n",
      "216000 of 715313 Tweets processed\n",
      "217000 of 715313 Tweets processed\n",
      "218000 of 715313 Tweets processed\n",
      "219000 of 715313 Tweets processed\n",
      "220000 of 715313 Tweets processed\n",
      "221000 of 715313 Tweets processed\n",
      "222000 of 715313 Tweets processed\n",
      "223000 of 715313 Tweets processed\n",
      "224000 of 715313 Tweets processed\n",
      "225000 of 715313 Tweets processed\n",
      "226000 of 715313 Tweets processed\n",
      "227000 of 715313 Tweets processed\n",
      "228000 of 715313 Tweets processed\n",
      "229000 of 715313 Tweets processed\n",
      "230000 of 715313 Tweets processed\n",
      "231000 of 715313 Tweets processed\n",
      "232000 of 715313 Tweets processed\n",
      "233000 of 715313 Tweets processed\n",
      "234000 of 715313 Tweets processed\n",
      "235000 of 715313 Tweets processed\n",
      "236000 of 715313 Tweets processed\n",
      "237000 of 715313 Tweets processed\n",
      "238000 of 715313 Tweets processed\n",
      "239000 of 715313 Tweets processed\n",
      "240000 of 715313 Tweets processed\n",
      "241000 of 715313 Tweets processed\n",
      "242000 of 715313 Tweets processed\n",
      "243000 of 715313 Tweets processed\n",
      "244000 of 715313 Tweets processed\n",
      "245000 of 715313 Tweets processed\n",
      "246000 of 715313 Tweets processed\n",
      "247000 of 715313 Tweets processed\n",
      "248000 of 715313 Tweets processed\n",
      "249000 of 715313 Tweets processed\n",
      "250000 of 715313 Tweets processed\n",
      "251000 of 715313 Tweets processed\n",
      "252000 of 715313 Tweets processed\n",
      "253000 of 715313 Tweets processed\n",
      "254000 of 715313 Tweets processed\n",
      "255000 of 715313 Tweets processed\n",
      "256000 of 715313 Tweets processed\n",
      "257000 of 715313 Tweets processed\n",
      "258000 of 715313 Tweets processed\n",
      "259000 of 715313 Tweets processed\n",
      "260000 of 715313 Tweets processed\n",
      "261000 of 715313 Tweets processed\n",
      "262000 of 715313 Tweets processed\n",
      "263000 of 715313 Tweets processed\n",
      "264000 of 715313 Tweets processed\n",
      "265000 of 715313 Tweets processed\n",
      "266000 of 715313 Tweets processed\n",
      "267000 of 715313 Tweets processed\n",
      "268000 of 715313 Tweets processed\n",
      "269000 of 715313 Tweets processed\n",
      "270000 of 715313 Tweets processed\n",
      "271000 of 715313 Tweets processed\n",
      "272000 of 715313 Tweets processed\n",
      "273000 of 715313 Tweets processed\n",
      "274000 of 715313 Tweets processed\n",
      "275000 of 715313 Tweets processed\n",
      "276000 of 715313 Tweets processed\n",
      "277000 of 715313 Tweets processed\n",
      "278000 of 715313 Tweets processed\n",
      "279000 of 715313 Tweets processed\n",
      "280000 of 715313 Tweets processed\n",
      "281000 of 715313 Tweets processed\n",
      "282000 of 715313 Tweets processed\n",
      "283000 of 715313 Tweets processed\n",
      "284000 of 715313 Tweets processed\n",
      "285000 of 715313 Tweets processed\n",
      "286000 of 715313 Tweets processed\n",
      "287000 of 715313 Tweets processed\n",
      "288000 of 715313 Tweets processed\n",
      "289000 of 715313 Tweets processed\n",
      "290000 of 715313 Tweets processed\n",
      "291000 of 715313 Tweets processed\n",
      "292000 of 715313 Tweets processed\n",
      "293000 of 715313 Tweets processed\n",
      "294000 of 715313 Tweets processed\n",
      "295000 of 715313 Tweets processed\n",
      "296000 of 715313 Tweets processed\n",
      "297000 of 715313 Tweets processed\n",
      "298000 of 715313 Tweets processed\n",
      "299000 of 715313 Tweets processed\n",
      "300000 of 715313 Tweets processed\n",
      "301000 of 715313 Tweets processed\n",
      "302000 of 715313 Tweets processed\n",
      "303000 of 715313 Tweets processed\n",
      "304000 of 715313 Tweets processed\n",
      "305000 of 715313 Tweets processed\n",
      "306000 of 715313 Tweets processed\n",
      "307000 of 715313 Tweets processed\n",
      "308000 of 715313 Tweets processed\n",
      "309000 of 715313 Tweets processed\n",
      "310000 of 715313 Tweets processed\n",
      "311000 of 715313 Tweets processed\n",
      "312000 of 715313 Tweets processed\n",
      "313000 of 715313 Tweets processed\n",
      "314000 of 715313 Tweets processed\n",
      "315000 of 715313 Tweets processed\n",
      "316000 of 715313 Tweets processed\n",
      "317000 of 715313 Tweets processed\n",
      "318000 of 715313 Tweets processed\n",
      "319000 of 715313 Tweets processed\n",
      "320000 of 715313 Tweets processed\n",
      "321000 of 715313 Tweets processed\n",
      "322000 of 715313 Tweets processed\n",
      "323000 of 715313 Tweets processed\n",
      "324000 of 715313 Tweets processed\n",
      "325000 of 715313 Tweets processed\n",
      "326000 of 715313 Tweets processed\n",
      "327000 of 715313 Tweets processed\n",
      "328000 of 715313 Tweets processed\n",
      "329000 of 715313 Tweets processed\n",
      "330000 of 715313 Tweets processed\n",
      "331000 of 715313 Tweets processed\n",
      "332000 of 715313 Tweets processed\n",
      "333000 of 715313 Tweets processed\n",
      "334000 of 715313 Tweets processed\n",
      "335000 of 715313 Tweets processed\n",
      "336000 of 715313 Tweets processed\n",
      "337000 of 715313 Tweets processed\n",
      "338000 of 715313 Tweets processed\n",
      "339000 of 715313 Tweets processed\n",
      "340000 of 715313 Tweets processed\n",
      "341000 of 715313 Tweets processed\n",
      "342000 of 715313 Tweets processed\n",
      "343000 of 715313 Tweets processed\n",
      "344000 of 715313 Tweets processed\n",
      "345000 of 715313 Tweets processed\n",
      "346000 of 715313 Tweets processed\n",
      "347000 of 715313 Tweets processed\n",
      "348000 of 715313 Tweets processed\n",
      "349000 of 715313 Tweets processed\n",
      "350000 of 715313 Tweets processed\n",
      "351000 of 715313 Tweets processed\n",
      "352000 of 715313 Tweets processed\n",
      "353000 of 715313 Tweets processed\n",
      "354000 of 715313 Tweets processed\n",
      "355000 of 715313 Tweets processed\n",
      "356000 of 715313 Tweets processed\n",
      "357000 of 715313 Tweets processed\n",
      "358000 of 715313 Tweets processed\n",
      "359000 of 715313 Tweets processed\n",
      "360000 of 715313 Tweets processed\n",
      "361000 of 715313 Tweets processed\n",
      "362000 of 715313 Tweets processed\n",
      "363000 of 715313 Tweets processed\n",
      "364000 of 715313 Tweets processed\n",
      "365000 of 715313 Tweets processed\n",
      "366000 of 715313 Tweets processed\n",
      "367000 of 715313 Tweets processed\n",
      "368000 of 715313 Tweets processed\n",
      "369000 of 715313 Tweets processed\n",
      "370000 of 715313 Tweets processed\n",
      "371000 of 715313 Tweets processed\n",
      "372000 of 715313 Tweets processed\n",
      "373000 of 715313 Tweets processed\n",
      "374000 of 715313 Tweets processed\n",
      "375000 of 715313 Tweets processed\n",
      "376000 of 715313 Tweets processed\n",
      "377000 of 715313 Tweets processed\n",
      "378000 of 715313 Tweets processed\n",
      "379000 of 715313 Tweets processed\n",
      "380000 of 715313 Tweets processed\n",
      "381000 of 715313 Tweets processed\n",
      "382000 of 715313 Tweets processed\n",
      "383000 of 715313 Tweets processed\n",
      "384000 of 715313 Tweets processed\n",
      "385000 of 715313 Tweets processed\n",
      "386000 of 715313 Tweets processed\n",
      "387000 of 715313 Tweets processed\n",
      "388000 of 715313 Tweets processed\n",
      "389000 of 715313 Tweets processed\n",
      "390000 of 715313 Tweets processed\n",
      "391000 of 715313 Tweets processed\n",
      "392000 of 715313 Tweets processed\n",
      "393000 of 715313 Tweets processed\n",
      "394000 of 715313 Tweets processed\n",
      "395000 of 715313 Tweets processed\n",
      "396000 of 715313 Tweets processed\n",
      "397000 of 715313 Tweets processed\n",
      "398000 of 715313 Tweets processed\n",
      "399000 of 715313 Tweets processed\n",
      "400000 of 715313 Tweets processed\n",
      "401000 of 715313 Tweets processed\n",
      "402000 of 715313 Tweets processed\n",
      "403000 of 715313 Tweets processed\n",
      "404000 of 715313 Tweets processed\n",
      "405000 of 715313 Tweets processed\n",
      "406000 of 715313 Tweets processed\n",
      "407000 of 715313 Tweets processed\n",
      "408000 of 715313 Tweets processed\n",
      "409000 of 715313 Tweets processed\n",
      "410000 of 715313 Tweets processed\n",
      "411000 of 715313 Tweets processed\n",
      "412000 of 715313 Tweets processed\n",
      "413000 of 715313 Tweets processed\n",
      "414000 of 715313 Tweets processed\n",
      "415000 of 715313 Tweets processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416000 of 715313 Tweets processed\n",
      "417000 of 715313 Tweets processed\n",
      "418000 of 715313 Tweets processed\n",
      "419000 of 715313 Tweets processed\n",
      "420000 of 715313 Tweets processed\n",
      "421000 of 715313 Tweets processed\n",
      "422000 of 715313 Tweets processed\n",
      "423000 of 715313 Tweets processed\n",
      "424000 of 715313 Tweets processed\n",
      "425000 of 715313 Tweets processed\n",
      "426000 of 715313 Tweets processed\n",
      "427000 of 715313 Tweets processed\n",
      "428000 of 715313 Tweets processed\n",
      "429000 of 715313 Tweets processed\n",
      "430000 of 715313 Tweets processed\n",
      "431000 of 715313 Tweets processed\n",
      "432000 of 715313 Tweets processed\n",
      "433000 of 715313 Tweets processed\n",
      "434000 of 715313 Tweets processed\n",
      "435000 of 715313 Tweets processed\n",
      "436000 of 715313 Tweets processed\n",
      "437000 of 715313 Tweets processed\n",
      "438000 of 715313 Tweets processed\n",
      "439000 of 715313 Tweets processed\n",
      "440000 of 715313 Tweets processed\n",
      "441000 of 715313 Tweets processed\n",
      "442000 of 715313 Tweets processed\n",
      "443000 of 715313 Tweets processed\n",
      "444000 of 715313 Tweets processed\n",
      "445000 of 715313 Tweets processed\n",
      "446000 of 715313 Tweets processed\n",
      "447000 of 715313 Tweets processed\n",
      "448000 of 715313 Tweets processed\n",
      "449000 of 715313 Tweets processed\n",
      "450000 of 715313 Tweets processed\n",
      "451000 of 715313 Tweets processed\n",
      "452000 of 715313 Tweets processed\n",
      "453000 of 715313 Tweets processed\n",
      "454000 of 715313 Tweets processed\n",
      "455000 of 715313 Tweets processed\n",
      "456000 of 715313 Tweets processed\n",
      "457000 of 715313 Tweets processed\n",
      "458000 of 715313 Tweets processed\n",
      "459000 of 715313 Tweets processed\n",
      "460000 of 715313 Tweets processed\n",
      "461000 of 715313 Tweets processed\n",
      "462000 of 715313 Tweets processed\n",
      "463000 of 715313 Tweets processed\n",
      "464000 of 715313 Tweets processed\n",
      "465000 of 715313 Tweets processed\n",
      "466000 of 715313 Tweets processed\n",
      "467000 of 715313 Tweets processed\n",
      "468000 of 715313 Tweets processed\n",
      "469000 of 715313 Tweets processed\n",
      "470000 of 715313 Tweets processed\n",
      "471000 of 715313 Tweets processed\n",
      "472000 of 715313 Tweets processed\n",
      "473000 of 715313 Tweets processed\n",
      "474000 of 715313 Tweets processed\n",
      "475000 of 715313 Tweets processed\n",
      "476000 of 715313 Tweets processed\n",
      "477000 of 715313 Tweets processed\n",
      "478000 of 715313 Tweets processed\n",
      "479000 of 715313 Tweets processed\n",
      "480000 of 715313 Tweets processed\n",
      "481000 of 715313 Tweets processed\n",
      "482000 of 715313 Tweets processed\n",
      "483000 of 715313 Tweets processed\n",
      "484000 of 715313 Tweets processed\n",
      "485000 of 715313 Tweets processed\n",
      "486000 of 715313 Tweets processed\n",
      "487000 of 715313 Tweets processed\n",
      "488000 of 715313 Tweets processed\n",
      "489000 of 715313 Tweets processed\n",
      "490000 of 715313 Tweets processed\n",
      "491000 of 715313 Tweets processed\n",
      "492000 of 715313 Tweets processed\n",
      "493000 of 715313 Tweets processed\n",
      "494000 of 715313 Tweets processed\n",
      "495000 of 715313 Tweets processed\n",
      "496000 of 715313 Tweets processed\n",
      "497000 of 715313 Tweets processed\n",
      "498000 of 715313 Tweets processed\n",
      "499000 of 715313 Tweets processed\n",
      "500000 of 715313 Tweets processed\n",
      "501000 of 715313 Tweets processed\n",
      "502000 of 715313 Tweets processed\n",
      "503000 of 715313 Tweets processed\n",
      "504000 of 715313 Tweets processed\n",
      "505000 of 715313 Tweets processed\n",
      "506000 of 715313 Tweets processed\n",
      "507000 of 715313 Tweets processed\n",
      "508000 of 715313 Tweets processed\n",
      "509000 of 715313 Tweets processed\n",
      "510000 of 715313 Tweets processed\n",
      "511000 of 715313 Tweets processed\n",
      "512000 of 715313 Tweets processed\n",
      "513000 of 715313 Tweets processed\n",
      "514000 of 715313 Tweets processed\n",
      "515000 of 715313 Tweets processed\n",
      "516000 of 715313 Tweets processed\n",
      "517000 of 715313 Tweets processed\n",
      "518000 of 715313 Tweets processed\n",
      "519000 of 715313 Tweets processed\n",
      "520000 of 715313 Tweets processed\n",
      "521000 of 715313 Tweets processed\n",
      "522000 of 715313 Tweets processed\n",
      "523000 of 715313 Tweets processed\n",
      "524000 of 715313 Tweets processed\n",
      "525000 of 715313 Tweets processed\n",
      "526000 of 715313 Tweets processed\n",
      "527000 of 715313 Tweets processed\n",
      "528000 of 715313 Tweets processed\n",
      "529000 of 715313 Tweets processed\n",
      "530000 of 715313 Tweets processed\n",
      "531000 of 715313 Tweets processed\n",
      "532000 of 715313 Tweets processed\n",
      "533000 of 715313 Tweets processed\n",
      "534000 of 715313 Tweets processed\n",
      "535000 of 715313 Tweets processed\n",
      "536000 of 715313 Tweets processed\n",
      "537000 of 715313 Tweets processed\n",
      "538000 of 715313 Tweets processed\n",
      "539000 of 715313 Tweets processed\n",
      "540000 of 715313 Tweets processed\n",
      "541000 of 715313 Tweets processed\n",
      "542000 of 715313 Tweets processed\n",
      "543000 of 715313 Tweets processed\n",
      "544000 of 715313 Tweets processed\n",
      "545000 of 715313 Tweets processed\n",
      "546000 of 715313 Tweets processed\n",
      "547000 of 715313 Tweets processed\n",
      "548000 of 715313 Tweets processed\n",
      "549000 of 715313 Tweets processed\n",
      "550000 of 715313 Tweets processed\n",
      "551000 of 715313 Tweets processed\n",
      "552000 of 715313 Tweets processed\n",
      "553000 of 715313 Tweets processed\n",
      "554000 of 715313 Tweets processed\n",
      "555000 of 715313 Tweets processed\n",
      "556000 of 715313 Tweets processed\n",
      "557000 of 715313 Tweets processed\n",
      "558000 of 715313 Tweets processed\n",
      "559000 of 715313 Tweets processed\n",
      "560000 of 715313 Tweets processed\n",
      "561000 of 715313 Tweets processed\n",
      "562000 of 715313 Tweets processed\n",
      "563000 of 715313 Tweets processed\n",
      "564000 of 715313 Tweets processed\n",
      "565000 of 715313 Tweets processed\n",
      "566000 of 715313 Tweets processed\n",
      "567000 of 715313 Tweets processed\n",
      "568000 of 715313 Tweets processed\n",
      "569000 of 715313 Tweets processed\n",
      "570000 of 715313 Tweets processed\n",
      "571000 of 715313 Tweets processed\n",
      "572000 of 715313 Tweets processed\n",
      "573000 of 715313 Tweets processed\n",
      "574000 of 715313 Tweets processed\n",
      "575000 of 715313 Tweets processed\n",
      "576000 of 715313 Tweets processed\n",
      "577000 of 715313 Tweets processed\n",
      "578000 of 715313 Tweets processed\n",
      "579000 of 715313 Tweets processed\n",
      "580000 of 715313 Tweets processed\n",
      "581000 of 715313 Tweets processed\n",
      "582000 of 715313 Tweets processed\n",
      "583000 of 715313 Tweets processed\n",
      "584000 of 715313 Tweets processed\n",
      "585000 of 715313 Tweets processed\n",
      "586000 of 715313 Tweets processed\n",
      "587000 of 715313 Tweets processed\n",
      "588000 of 715313 Tweets processed\n",
      "589000 of 715313 Tweets processed\n",
      "590000 of 715313 Tweets processed\n",
      "591000 of 715313 Tweets processed\n",
      "592000 of 715313 Tweets processed\n",
      "593000 of 715313 Tweets processed\n",
      "594000 of 715313 Tweets processed\n",
      "595000 of 715313 Tweets processed\n",
      "596000 of 715313 Tweets processed\n",
      "597000 of 715313 Tweets processed\n",
      "598000 of 715313 Tweets processed\n",
      "599000 of 715313 Tweets processed\n",
      "600000 of 715313 Tweets processed\n",
      "601000 of 715313 Tweets processed\n",
      "602000 of 715313 Tweets processed\n",
      "603000 of 715313 Tweets processed\n",
      "604000 of 715313 Tweets processed\n",
      "605000 of 715313 Tweets processed\n",
      "606000 of 715313 Tweets processed\n",
      "607000 of 715313 Tweets processed\n",
      "608000 of 715313 Tweets processed\n",
      "609000 of 715313 Tweets processed\n",
      "610000 of 715313 Tweets processed\n",
      "611000 of 715313 Tweets processed\n",
      "612000 of 715313 Tweets processed\n",
      "613000 of 715313 Tweets processed\n",
      "614000 of 715313 Tweets processed\n",
      "615000 of 715313 Tweets processed\n",
      "616000 of 715313 Tweets processed\n",
      "617000 of 715313 Tweets processed\n",
      "618000 of 715313 Tweets processed\n",
      "619000 of 715313 Tweets processed\n",
      "620000 of 715313 Tweets processed\n",
      "621000 of 715313 Tweets processed\n",
      "622000 of 715313 Tweets processed\n",
      "623000 of 715313 Tweets processed\n",
      "624000 of 715313 Tweets processed\n",
      "625000 of 715313 Tweets processed\n",
      "626000 of 715313 Tweets processed\n",
      "627000 of 715313 Tweets processed\n",
      "628000 of 715313 Tweets processed\n",
      "629000 of 715313 Tweets processed\n",
      "630000 of 715313 Tweets processed\n",
      "631000 of 715313 Tweets processed\n",
      "632000 of 715313 Tweets processed\n",
      "633000 of 715313 Tweets processed\n",
      "634000 of 715313 Tweets processed\n",
      "635000 of 715313 Tweets processed\n",
      "636000 of 715313 Tweets processed\n",
      "637000 of 715313 Tweets processed\n",
      "638000 of 715313 Tweets processed\n",
      "639000 of 715313 Tweets processed\n",
      "640000 of 715313 Tweets processed\n",
      "641000 of 715313 Tweets processed\n",
      "642000 of 715313 Tweets processed\n",
      "643000 of 715313 Tweets processed\n",
      "644000 of 715313 Tweets processed\n",
      "645000 of 715313 Tweets processed\n",
      "646000 of 715313 Tweets processed\n",
      "647000 of 715313 Tweets processed\n",
      "648000 of 715313 Tweets processed\n",
      "649000 of 715313 Tweets processed\n",
      "650000 of 715313 Tweets processed\n",
      "651000 of 715313 Tweets processed\n",
      "652000 of 715313 Tweets processed\n",
      "653000 of 715313 Tweets processed\n",
      "654000 of 715313 Tweets processed\n",
      "655000 of 715313 Tweets processed\n",
      "656000 of 715313 Tweets processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657000 of 715313 Tweets processed\n",
      "658000 of 715313 Tweets processed\n",
      "659000 of 715313 Tweets processed\n",
      "660000 of 715313 Tweets processed\n",
      "661000 of 715313 Tweets processed\n",
      "662000 of 715313 Tweets processed\n",
      "663000 of 715313 Tweets processed\n",
      "664000 of 715313 Tweets processed\n",
      "665000 of 715313 Tweets processed\n",
      "666000 of 715313 Tweets processed\n",
      "667000 of 715313 Tweets processed\n",
      "668000 of 715313 Tweets processed\n",
      "669000 of 715313 Tweets processed\n",
      "670000 of 715313 Tweets processed\n",
      "671000 of 715313 Tweets processed\n",
      "672000 of 715313 Tweets processed\n",
      "673000 of 715313 Tweets processed\n",
      "674000 of 715313 Tweets processed\n",
      "675000 of 715313 Tweets processed\n",
      "676000 of 715313 Tweets processed\n",
      "677000 of 715313 Tweets processed\n",
      "678000 of 715313 Tweets processed\n",
      "679000 of 715313 Tweets processed\n",
      "680000 of 715313 Tweets processed\n",
      "681000 of 715313 Tweets processed\n",
      "682000 of 715313 Tweets processed\n",
      "683000 of 715313 Tweets processed\n",
      "684000 of 715313 Tweets processed\n",
      "685000 of 715313 Tweets processed\n",
      "686000 of 715313 Tweets processed\n",
      "687000 of 715313 Tweets processed\n",
      "688000 of 715313 Tweets processed\n",
      "689000 of 715313 Tweets processed\n",
      "690000 of 715313 Tweets processed\n",
      "691000 of 715313 Tweets processed\n",
      "692000 of 715313 Tweets processed\n",
      "693000 of 715313 Tweets processed\n",
      "694000 of 715313 Tweets processed\n",
      "695000 of 715313 Tweets processed\n",
      "696000 of 715313 Tweets processed\n",
      "697000 of 715313 Tweets processed\n",
      "698000 of 715313 Tweets processed\n",
      "699000 of 715313 Tweets processed\n",
      "700000 of 715313 Tweets processed\n",
      "701000 of 715313 Tweets processed\n",
      "702000 of 715313 Tweets processed\n",
      "703000 of 715313 Tweets processed\n",
      "704000 of 715313 Tweets processed\n",
      "705000 of 715313 Tweets processed\n",
      "706000 of 715313 Tweets processed\n",
      "707000 of 715313 Tweets processed\n",
      "708000 of 715313 Tweets processed\n",
      "709000 of 715313 Tweets processed\n",
      "710000 of 715313 Tweets processed\n",
      "711000 of 715313 Tweets processed\n",
      "712000 of 715313 Tweets processed\n",
      "713000 of 715313 Tweets processed\n",
      "714000 of 715313 Tweets processed\n",
      "715000 of 715313 Tweets processed\n",
      "715313 of 715313 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#path = \"../../train_data/twitter_es/2019_03/tweets_{}_14_to_31.tsv\"\n",
    "path = \"../../train_data/twitter_en/2018/10/01/tweets_{}_01.tsv\"\n",
    "\n",
    "origin_path = path.format(\"clean\")\n",
    "save_path = path.format(\"processed\")\n",
    "\n",
    "#with open(origin_path) as fin:\n",
    "#    lines = fin.readlines()\n",
    "#print(lines[0])\n",
    "\n",
    "vocab = preprocess_file(origin_path, save_path, \"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary length sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217598\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the preprocessed dataset for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "1000 of 1000 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "5149 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 1000 Tweets processed\n",
      "1000 of 1000 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n",
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "9000 of 9000 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "18075 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 9000 Tweets processed\n",
      "2000 of 9000 Tweets processed\n",
      "3000 of 9000 Tweets processed\n",
      "4000 of 9000 Tweets processed\n",
      "5000 of 9000 Tweets processed\n",
      "6000 of 9000 Tweets processed\n",
      "7000 of 9000 Tweets processed\n",
      "8000 of 9000 Tweets processed\n",
      "9000 of 9000 Tweets processed\n",
      "9000 of 9000 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n",
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "3000 of 3000 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "8862 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 3000 Tweets processed\n",
      "2000 of 3000 Tweets processed\n",
      "3000 of 3000 Tweets processed\n",
      "3000 of 3000 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "origin = '../../data/hateval2019/hateval2019_en_'\n",
    "save = '../../data/hateval2019/hateval2019_clean_en_'\n",
    "files = [\"dev\", \"train\", \"test\"]\n",
    "\n",
    "for file in files:\n",
    "    origin_path = origin + file + \".csv\"\n",
    "    save_path = save + file + \".csv\"\n",
    "\n",
    "    preprocess_file(origin_path, save_path, \"DataFrame\")#, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 15, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-055781a07cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/embed_bias/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/embed_bias/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/embed_bias/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/embed_bias/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 15, saw 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(save_path, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text  HS  TR  AG\n",
      "0  34243  <MENTION> <MENTION> <MENTION> oh , i could hav...   0   0   0\n",
      "1  30593  several of the wild fires in <HASH> california...   0   0   0\n",
      "2  31427  <MENTION> my question is how do you resettle a...   0   0   0\n",
      "3  31694  <HASH> europe , you've got a problem ! we must...   1   0   0\n",
      "4  31865  this is outrageous ! <HASH> stopillegalimmigra...   1   0   0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed_bias",
   "language": "python",
   "name": "embed_bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
