{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Join the Raw Twitter TSV Files\n",
    "\n",
    "The `filter_by_language` notebook takes each day's tweets and, well, filters them by language. This notebook takes all of the days within a month and joins them. If you have another month's data, join that first and rename it to `00_clean.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Preprocessing import fetch_longtweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and get a set of the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/seraphinagoldfarb-tarrant/PycharmProjects/embedding_bias/scripts/data_cleaning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "head = set([])\n",
    "\n",
    "path = \"../../train_data/twitter_es/2019_04/\"\n",
    "\n",
    "for a in range(0,4):\n",
    "    for b in range(0,10):\n",
    "        day = str(a) + str(b)\n",
    "        if int(day)>31:\n",
    "            break\n",
    "        #print(day)\n",
    "        try:\n",
    "            df = pd.read_csv(path+str(day)+\"_clean.tsv\", index_col=\"id\", sep=\"\\t\", dtype=str).fillna(\"\")\n",
    "            data.append(df)\n",
    "            head = head.union(set(df.columns))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            #print(\"oh, no\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is if there is just one file\n",
    "path = \"../../train_data/twitter_en/2018/10/01/\"\n",
    "df = pd.read_csv(path+\"01_clean.tsv\",index_col=\"id\", sep=\"\\t\", dtype=str).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = set([])\n",
    "head = head.union(set(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data for all of the days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(873762, 36)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(data, sort=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all of the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coordinates', 'quote_count', 'lang', 'in_reply_to_user_id', 'is_quote_status', 'extended_entities', 'in_reply_to_status_id', 'id_str', 'favorited', 'geo', 'filter_level', 'reply_count', 'quoted_status_id', 'extended_tweet', 'retweet_count', 'contributors', 'place', 'withheld_in_countries', 'favorite_count', 'in_reply_to_screen_name', 'possibly_sensitive', 'quoted_status_id_str', 'in_reply_to_status_id_str', 'display_text_range', 'entities', 'timestamp_ms', 'retweeted', 'retweeted_status', 'created_at', 'quoted_status_permalink', 'text', 'quoted_status', 'truncated', 'user', 'source', 'in_reply_to_user_id_str'}\n"
     ]
    }
   ],
   "source": [
    "#print(set(df.columns))\n",
    "print(head)\n",
    "hds = set(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a sanity check to see that no data was excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(head.symmetric_difference(set(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to a tsv file. This should be the one used to preprocess the data and any other sort of manipulation that we want to do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+\"no_RTs.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 873761 Tweets fetched...\n",
      "10000 of 873761 Tweets fetched...\n",
      "20000 of 873761 Tweets fetched...\n",
      "30000 of 873761 Tweets fetched...\n",
      "40000 of 873761 Tweets fetched...\n",
      "50000 of 873761 Tweets fetched...\n",
      "60000 of 873761 Tweets fetched...\n",
      "70000 of 873761 Tweets fetched...\n",
      "80000 of 873761 Tweets fetched...\n",
      "90000 of 873761 Tweets fetched...\n",
      "100000 of 873761 Tweets fetched...\n",
      "110000 of 873761 Tweets fetched...\n",
      "120000 of 873761 Tweets fetched...\n",
      "130000 of 873761 Tweets fetched...\n",
      "140000 of 873761 Tweets fetched...\n",
      "150000 of 873761 Tweets fetched...\n",
      "160000 of 873761 Tweets fetched...\n",
      "170000 of 873761 Tweets fetched...\n",
      "180000 of 873761 Tweets fetched...\n",
      "190000 of 873761 Tweets fetched...\n",
      "200000 of 873761 Tweets fetched...\n",
      "210000 of 873761 Tweets fetched...\n",
      "220000 of 873761 Tweets fetched...\n",
      "230000 of 873761 Tweets fetched...\n",
      "240000 of 873761 Tweets fetched...\n",
      "250000 of 873761 Tweets fetched...\n",
      "260000 of 873761 Tweets fetched...\n",
      "270000 of 873761 Tweets fetched...\n",
      "280000 of 873761 Tweets fetched...\n",
      "290000 of 873761 Tweets fetched...\n",
      "300000 of 873761 Tweets fetched...\n",
      "310000 of 873761 Tweets fetched...\n",
      "320000 of 873761 Tweets fetched...\n",
      "330000 of 873761 Tweets fetched...\n",
      "340000 of 873761 Tweets fetched...\n",
      "350000 of 873761 Tweets fetched...\n",
      "360000 of 873761 Tweets fetched...\n",
      "370000 of 873761 Tweets fetched...\n",
      "380000 of 873761 Tweets fetched...\n",
      "390000 of 873761 Tweets fetched...\n",
      "400000 of 873761 Tweets fetched...\n",
      "410000 of 873761 Tweets fetched...\n",
      "420000 of 873761 Tweets fetched...\n",
      "430000 of 873761 Tweets fetched...\n",
      "440000 of 873761 Tweets fetched...\n",
      "450000 of 873761 Tweets fetched...\n",
      "460000 of 873761 Tweets fetched...\n",
      "470000 of 873761 Tweets fetched...\n",
      "480000 of 873761 Tweets fetched...\n",
      "490000 of 873761 Tweets fetched...\n",
      "500000 of 873761 Tweets fetched...\n",
      "510000 of 873761 Tweets fetched...\n",
      "520000 of 873761 Tweets fetched...\n",
      "530000 of 873761 Tweets fetched...\n",
      "540000 of 873761 Tweets fetched...\n",
      "550000 of 873761 Tweets fetched...\n",
      "560000 of 873761 Tweets fetched...\n",
      "570000 of 873761 Tweets fetched...\n",
      "580000 of 873761 Tweets fetched...\n",
      "590000 of 873761 Tweets fetched...\n",
      "600000 of 873761 Tweets fetched...\n",
      "610000 of 873761 Tweets fetched...\n",
      "620000 of 873761 Tweets fetched...\n",
      "630000 of 873761 Tweets fetched...\n",
      "640000 of 873761 Tweets fetched...\n",
      "650000 of 873761 Tweets fetched...\n",
      "660000 of 873761 Tweets fetched...\n",
      "670000 of 873761 Tweets fetched...\n",
      "680000 of 873761 Tweets fetched...\n",
      "690000 of 873761 Tweets fetched...\n",
      "700000 of 873761 Tweets fetched...\n",
      "710000 of 873761 Tweets fetched...\n",
      "720000 of 873761 Tweets fetched...\n",
      "730000 of 873761 Tweets fetched...\n",
      "740000 of 873761 Tweets fetched...\n",
      "750000 of 873761 Tweets fetched...\n",
      "760000 of 873761 Tweets fetched...\n",
      "770000 of 873761 Tweets fetched...\n",
      "780000 of 873761 Tweets fetched...\n",
      "790000 of 873761 Tweets fetched...\n",
      "800000 of 873761 Tweets fetched...\n",
      "810000 of 873761 Tweets fetched...\n",
      "820000 of 873761 Tweets fetched...\n",
      "830000 of 873761 Tweets fetched...\n",
      "840000 of 873761 Tweets fetched...\n",
      "850000 of 873761 Tweets fetched...\n",
      "860000 of 873761 Tweets fetched...\n",
      "870000 of 873761 Tweets fetched...\n",
      "873761 of 873761 Tweets fetched...\n"
     ]
    }
   ],
   "source": [
    "tweets = fetch_longtweet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(path+\"tweets_clean_all.tsv\",sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed_bias",
   "language": "python",
   "name": "embed_bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
