name,weat_es_hatespeech_gender,precision_diff,recall_diff,f1_diff
original1,0.3705964238237563,-0.0031908831908832314,-0.03613348576215858,-0.0193881792607411
original2,0.3705964238237563,0.008205844210693702,-0.0345517458045711,-0.01457282576090524
original3,0.3705964238237563,0.004262557781201881,-0.028861845377828543,-0.013321718339488253
original4,0.3705964238237563,0.0021726239889945376,-0.02230580596741405,-0.010581794039271397
original5,0.3705964238237563,0.004459706959706988,-0.03304200228643239,-0.014743192291579388
original6,0.3705964238237563,0.006673137587964173,-0.020549529186920212,-0.007530130769703769
original7,0.3705964238237563,-0.002315060177110473,-0.02339447940029149,-0.013384347507331307
original8,0.3705964238237563,0.010269931909067997,-0.029219645864786314,-0.009785932721712687
original9,0.3705964238237563,-0.0010103867760579144,-0.032398397751965735,-0.016929423116076148
original10,0.3705964238237563,0.005637994634473453,-0.016727172765274245,-0.005910569678191702
db_hatespeech_gender_debias_0.0,-0.6636159663777833,0.009020466106707703,-0.043420398118492964,-0.01880971406733145
db_hatespeech_gender_debias_0.1,-0.6200810653560724,0.007170772559145777,-0.029839251586103344,-0.012576968426568857
db_hatespeech_gender_debias_0.2,-0.4733085061713127,0.008423747409484372,-0.032040597265007964,-0.012939692919922141
db_hatespeech_gender_debias_0.3,-0.44031694189640086,0.011677804432972394,-0.027105568597334817,-0.009027182938554423
db_hatespeech_gender_debias_0.4,-0.4569410756861798,-0.005548755609955158,-0.020350993550864338,-0.013092161546333636
db_hatespeech_gender_debias_0.5,-0.24165111647878262,-0.006787252368647634,-0.033264536735637784,-0.020368540553000858
db_hatespeech_gender_debias_0.6,0.017020286046590973,0.004228535886855345,-0.034353210168515336,-0.015660252156602716
db_hatespeech_gender_debias_0.7,-0.015437346917431429,0.006237312512328508,-0.02119313372138687,-0.008410991636798149
db_hatespeech_gender_debias_0.8,0.07981665944135154,0.0008636377656457972,-0.014279293824014494,-0.007339200122930767
db_hatespeech_gender_debias_0.9,0.20187932485189028,0.006016619900712228,-0.03195332885355484,-0.013253260169136638
db_hatespeech_gender_overbias_0.0,0.9188956165997159,0.009648334260037195,-0.03488554747837924,-0.013915302676019703
db_hatespeech_gender_overbias_0.1,0.9602291323505476,0.000652135293337186,-0.029330913089389066,-0.01456603104998555
db_hatespeech_gender_overbias_0.2,0.9442135035804996,0.0006553876922245072,-0.028019705207306123,-0.013910067732898379
db_hatespeech_gender_overbias_0.3,0.6439166033918047,-0.003353125420341607,-0.019015786855631878,-0.011567193417040711
db_hatespeech_gender_overbias_0.4,0.8305543773191216,-0.003086583267493892,-0.03775449650490004,-0.02089279291090551
db_hatespeech_gender_overbias_0.5,0.4932592536070536,0.002832480048718722,-0.03139699273054131,-0.014959971783481185
db_hatespeech_gender_overbias_0.6,0.5418082294978482,0.0010935143288084426,-0.03457574461772073,-0.01743355892098697
db_hatespeech_gender_overbias_0.7,0.300104815638272,0.0013198376144617496,-0.036443288622817205,-0.017959376434054652
db_hatespeech_gender_overbias_0.8,0.43159342080876556,-0.002018943170488563,-0.02768590353349798,-0.015096013245276096
db_hatespeech_gender_overbias_0.9,0.23909717984712878,0.005198017069104388,-0.04126705006588771,-0.01817166357240363
ar_hatespeech_gender_debias_reg1e-1_sim0.0_ant0.0,-0.10761360453331664,0.006253093548919342,-0.024062082747907776,-0.009492724058207558
ar_hatespeech_gender_debias_reg1e-1_sim0.0_ant1.0,-0.21721521015944736,-0.0037555448507941014,-0.030530853746869258,-0.017448204304314374
ar_hatespeech_gender_debias_reg1e-1_sim1.0_ant0.0,-0.16989827654288348,-0.007778861845915919,-0.035577149639145156,-0.02174241826455192
ar_hatespeech_gender_debias_reg1e-1_sim1.0_ant1.0,-0.2209553779711704,0.0010106949906281137,-0.022973409315030224,-0.011291506638376303
ar_hatespeech_gender_debias_reg5e-2_sim0.0_ant0.0,-0.5483446575974559,-0.006224350205198403,-0.026263428426812285,-0.01646605353808872
ar_hatespeech_gender_debias_reg5e-2_sim0.0_ant1.0,-0.7243988478789926,-0.0024412126313924087,-0.035465882414542405,-0.019078205474844845
ar_hatespeech_gender_debias_reg5e-2_sim1.0_ant0.0,-0.6475343176783266,0.004553829040219726,-0.030085784848458363,-0.01329036956780738
ar_hatespeech_gender_debias_reg5e-2_sim1.0_ant1.0,-0.8574261379558394,0.002679183179191136,-0.02155093420834464,-0.009693530256492555
ar_hatespeech_gender_debias_reg1e-2_sim0.0_ant0.0,-1.8431237351218288,0.0017385073639680781,-0.030419586522266506,-0.014754193744740962
ar_hatespeech_gender_debias_reg1e-2_sim0.0_ant1.0,-1.9156075473286336,-0.0009181979506555082,-0.030308319297663755,-0.015969636127377118
ar_hatespeech_gender_debias_reg1e-2_sim1.0_ant0.0,-1.8908656784427669,-0.003231969104566579,-0.04082198116747682,-0.02245154295097196
ar_hatespeech_gender_debias_reg1e-2_sim1.0_ant1.0,-1.9527916137905452,-3.5866156470554245e-05,-0.03808829817870829,-0.01931855494467205
ar_hatespeech_gender_overbias_reg1e-1_sim0.0_ant0.0,0.8576516394512055,0.006740760820457159,-0.021884735882152673,-0.007821096984599096
ar_hatespeech_gender_overbias_reg1e-1_sim0.0_ant1.0,0.9307138444501606,0.005955229799395556,-0.030530853746869258,-0.012686214874227497
ar_hatespeech_gender_overbias_reg1e-1_sim1.0_ant0.0,0.8985794104133239,-0.0009095149253731671,-0.027129567410484445,-0.014459534724304723
ar_hatespeech_gender_overbias_reg1e-1_sim1.0_ant1.0,1.0007767421647378,-0.005863131577417358,-0.021105865309933747,-0.013731237072425961
ar_hatespeech_gender_overbias_reg5e-2_sim0.0_ant0.0,1.17727742199715,0.003021817240679847,-0.03139699273054131,-0.014761998703604506
ar_hatespeech_gender_overbias_reg5e-2_sim0.0_ant1.0,1.290819092663815,0.0032514607488475233,-0.026239429613662657,-0.012229098887102663
ar_hatespeech_gender_overbias_reg5e-2_sim1.0_ant0.0,1.2275553112881692,0.0016462768175158349,-0.02941818150084219,-0.014575030704062963
ar_hatespeech_gender_overbias_reg5e-2_sim1.0_ant1.0,1.348003896341452,-0.006585441520715274,-0.028218240843361997,-0.017809608315375813
ar_hatespeech_gender_overbias_reg1e-2_sim0.0_ant0.0,1.803665788154378,-0.0017129629629629717,-0.02132839975913925,-0.011813234957110064
ar_hatespeech_gender_overbias_reg1e-2_sim0.0_ant1.0,1.9000858580773181,-0.0029436663511928085,-0.02439588442171592,-0.013882399533656442
ar_hatespeech_gender_overbias_reg1e-2_sim1.0_ant0.0,1.893834473112543,-0.0014678729689807657,-0.01848344954576797,-0.010216718266253921
ar_hatespeech_gender_overbias_reg1e-2_sim1.0_ant1.0,1.947736760348169,-0.0012987012987012436,-0.025286022218537596,-0.013304563561394867
