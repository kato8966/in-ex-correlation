{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Filter Tweets by Language\n",
    "\n",
    "This notebook has code to filter the archive.org twitter dumps by language and stores in two tsv files. One of them has all of the Tweets and the other one excludes Retweets. To use it, extract the tar file for a given day and a folder with a number `XX` will appear.\n",
    "\n",
    "This file must be in the same directory as that one. Within that folder there will be one for each hour of the day and within those there will be a json.bz2 file for each minute of the hour. Those must be extracted so the json files for each minute are in the corresponding hour directory.\n",
    "\n",
    "This piece of code imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these according to the name of the day and to the laguage that you want to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dir  = \"02\"\n",
    "language = \"es\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports the Tweets and filters them by language.\n",
    "\n",
    "If `Not Found` is displayed, the json file was most likely not extracted correctly. Some of the json files of the beginning of the 00 hour might be missing, which is expected. Any other of these mistakes is not.\n",
    "\n",
    "The messages `Backslash Character Found` and `Tab Character Found` appear to tell you that you should be careful with the output so that no rogue character breaks the final tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing data from hour 00\n",
      "Not Found: Hour: 00 Minute: 00\n",
      "Not Found: Hour: 00 Minute: 01\n",
      "Not Found: Hour: 00 Minute: 02\n",
      "Not Found: Hour: 00 Minute: 03\n",
      "Not Found: Hour: 00 Minute: 04\n",
      "Not Found: Hour: 00 Minute: 05\n",
      "Not Found: Hour: 00 Minute: 06\n",
      "Not Found: Hour: 00 Minute: 07\n",
      "Not Found: Hour: 00 Minute: 08\n",
      "Not Found: Hour: 00 Minute: 09\n",
      "Not Found: Hour: 00 Minute: 10\n",
      "Not Found: Hour: 00 Minute: 11\n",
      "Not Found: Hour: 00 Minute: 12\n",
      "Not Found: Hour: 00 Minute: 13\n",
      "Not Found: Hour: 00 Minute: 14\n",
      "Not Found: Hour: 00 Minute: 15\n",
      "Not Found: Hour: 00 Minute: 16\n",
      "Not Found: Hour: 00 Minute: 17\n",
      "Not Found: Hour: 00 Minute: 18\n",
      "Not Found: Hour: 00 Minute: 19\n",
      "Not Found: Hour: 00 Minute: 20\n",
      "Not Found: Hour: 00 Minute: 21\n",
      "Not Found: Hour: 00 Minute: 22\n",
      "Not Found: Hour: 00 Minute: 23\n",
      "Not Found: Hour: 00 Minute: 24\n",
      "Not Found: Hour: 00 Minute: 25\n",
      "Not Found: Hour: 00 Minute: 26\n",
      "Not Found: Hour: 00 Minute: 27\n",
      "Not Found: Hour: 00 Minute: 28\n",
      "\n",
      "Importing data from hour 01\n",
      "\n",
      "Importing data from hour 02\n",
      "\n",
      "Importing data from hour 03\n",
      "\n",
      "Importing data from hour 04\n",
      "\n",
      "Importing data from hour 05\n",
      "\n",
      "Importing data from hour 06\n",
      "\n",
      "Importing data from hour 07\n",
      "\n",
      "Importing data from hour 08\n",
      "\n",
      "Importing data from hour 09\n",
      "\n",
      "Importing data from hour 10\n",
      "\n",
      "Importing data from hour 11\n",
      "\n",
      "Importing data from hour 12\n",
      "\n",
      "Importing data from hour 13\n",
      "\n",
      "Importing data from hour 14\n",
      "\n",
      "Importing data from hour 15\n",
      "\n",
      "Importing data from hour 16\n",
      "\n",
      "Importing data from hour 17\n",
      "\n",
      "Importing data from hour 18\n",
      "\n",
      "Importing data from hour 19\n",
      "\n",
      "Importing data from hour 20\n",
      "\n",
      "Importing data from hour 21\n",
      "\n",
      "Importing data from hour 22\n",
      "\n",
      "Importing data from hour 23\n",
      "\n",
      "Total Tweets found:\n",
      "1026894\n"
     ]
    }
   ],
   "source": [
    "#path = \"./\" + day_dir + \"/\"\n",
    "language = \"en\"\n",
    "path = \"../../train_data/twitter_en/2018/10/01/\"\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for a in range(0,3):\n",
    "    for b in range(0,10):\n",
    "        if a*10 + b > 23:\n",
    "            continue\n",
    "        hour = str(a) + str(b)\n",
    "        print(\"\\nImporting data from hour\", hour)\n",
    "        for d in range(0,6):\n",
    "            for u in range(0,10):\n",
    "                minute = str(d) + str(u)\n",
    "                file = path + hour + \"/\" + minute + \".json\"\n",
    "                try:\n",
    "                    for line in open(file, 'r'):\n",
    "                        tweet = json.loads(line)\n",
    "                        if (\"lang\" in tweet.keys()) and (tweet[\"lang\"]==language):\n",
    "                            tweets.append(tweet)\n",
    "#                             if (\"\\t\" in tweet[\"text\"]):\n",
    "#                                 print(\"Tab character found!\")\n",
    "#                             if (\"\\\\\" in tweet[\"text\"]):\n",
    "#                                 print(\"Backslash found\")\n",
    "                except FileNotFoundError:\n",
    "                    print(\"Not Found: Hour: {} Minute: {}\".format(hour, minute))\n",
    "\n",
    "print(\"\\nTotal Tweets found:\")\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part checks whether all metadata is accounted for. The `keep` list is the one I would consider useful and the ones in `special` are the ones that could be useful to either further prune the data or to actually keept the proper fields. What you do with these is up to you. An output other than `[]` means that at least one of your entries has metadata that hadn't appeared on any of my runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nokeep = [\"user\", \"geo\", \"coordinates\", \"quote_count\", \"contributors\",\"reply_count\",\"retweet_count\", \"favorited\",\n",
    "          \"retweeted\", \"in_reply_to_status_id\", \"in_reply_to_status_id_str\", \"id_str\", \"created_at\", \"favorite_count\",\n",
    "          \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \"in_reply_to_screen_name\", \"display_text_range\", \"source\",\n",
    "          \"timestamp_ms\", \"retweeted_status\", \"entities\", \"extended_entities\", \"delete\", \"truncated\", \"is_quote_status\",\n",
    "          \"extended_tweet\", \"filter_level\", \"possibly_sensitive\", \"quoted_status_id\", \"quoted_status_id_str\",\n",
    "          \"quoted_status\", \"quoted_status_permalink\", \"TR\", \"DE\", \"withheld_in_countries\"]\n",
    "\n",
    "special = [\"truncated\", \"is_quote_status\", \"extended_tweet\"]\n",
    "\n",
    "keep = [\"id\", \"text\", \"lang\", \"place\"]\n",
    "\n",
    "other = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    for key in tweet.keys():\n",
    "        if key not in nokeep+keep:\n",
    "            check = \"withheld_in_countries\"\n",
    "            if key==check and tweet[check]!=False:\n",
    "                print(tweet[check])\n",
    "            if key not in other:\n",
    "                other.append(key)\n",
    "            \n",
    "print(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the Tweets in a `XX.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets)\n",
    "df = df.set_index(\"id\")    \n",
    "df = df.fillna(\"\")\n",
    "df.to_csv(path+\"01.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This other part filters Retweets and then saves them into a `XX_clean.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = df.loc[pd.notnull(df[\"retweeted_status\"])]\n",
    "#df_clean = df_clean.loc[df[\"retweeted_status\"]==\"\"]\n",
    "df_clean.to_csv(path+\"01_clean.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both numbers are different, that means that there were no Retweets in that minute. Depending on the language that you are dealing with, that might be highly unlikely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026894\n",
      "418106\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed_bias",
   "language": "python",
   "name": "embed_bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
